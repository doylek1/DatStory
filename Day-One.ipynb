{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day One: SESSION 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is the Natural Language Toolkit (NLTK)?\n",
    "NLTK is a Python Library for working with written language data. It is free, open source and well documented. The toolkit supports at least 40 different languages and is now used in university courses around the world.\n",
    "\n",
    "There is an extensive and accessible companion book available free [online](http://www.nltk.org/book/). Many areas covered in this chapter are covered in more detail in the book.\n",
    "\n",
    "_Note: NLTK provides tools for tasks ranging from very simple (counting words in a text) to very complex (writing and training parsers, etc.). Many advanced tasks are beyond the scope of this chapter_\n",
    "\n",
    "NLTK is a power tool. Most only use a fraction of its functions for a specific purpose. We will be using NLTK to preprocess and analysis Twitter data. Although we will use methods relevant to Twitter data, some of the methods and techniques used will be applicable to a variety of data. You will learn to analyse this data with some basic textual analytic tools, including counting word frequencies, concordancing and collocational analysis. \n",
    "\n",
    "Many other forms of analysis are possible in NLTK, including sentiment analysis. There are beyond the scope of this workshop, but many of the concept learnt here will help you if you wish to extend your knowledge later.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install instructions\n",
    "\n",
    "_**Install NLTK:**_ Open the Jupyter Notebook on your machine and install NLTK by typing the following commands into a fresh Jupyter Notebook.\n",
    "\n",
    "![](http://localhost:8888/files/Documents/DatStory/images/nltk_download.png)\n",
    "\n",
    "\n",
    "\n",
    "You can also follow the instructions on the nltk.org [site](http://www.nltk.org/install.html).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Textual Analysis with NLTK\n",
    "Before we dive into processing an analysing your own data, we'll give you can idea of what NLTK can do by working on some pre-processed practice data that comes with NLTK. \n",
    "\n",
    "Often it is easier (computationally) to analyse our data, then to upload and pre-process our data. So we'll start back-the-front.\n",
    "\n",
    "Start by importing NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy\n",
    "%matplotlib inline\n",
    "import pylab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll import the example corpora from the nltk book module (the * means 'the lot')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.book import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the book has assigned variable names to ten corpora. We can 'call' these names easily:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Texts ``text1, text2, text3`` etc... are call _**variables**_. In computer programming, variables are data (e.g. “Sense and Sensibility by Jane Austen 1811”) paired with an associated symbolic name or identifier (e.g. 'text2' in the code above). Variables are assigned in python with the ``=`` sign.\n",
    "\n",
    "Try assigning and calling your own variable. Here's my example below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python syntax\n",
    "The syntax of the Python programming language is the set of rules that defines how a Python program will be written and interpreted (by both computers and by human readers).\n",
    "\n",
    "Python was designed to be a highly readable language. It has a relatively uncluttered visual layout and uses English keywords frequently where other languages use punctuation. So it's really the perfect computer language for our purposes.\n",
    "The syntax we'll use the most are two types of commands; one that looks like this ``len(something)`` and another that look like this ``something.count()``\n",
    "\n",
    "Both need an object (text data in our case) to work on; for example, ``len(text1)`` or ``text1.count(\"Whale\")``.\n",
    "\n",
    "## Exploring vocabulary\n",
    "NLTK makes it really easy to get basic information about the size of a text and the complexity of its vocabulary.\n",
    "``len(text1)`` gives the number of symbols or 'tokens' in your text. This is the total number of words and items of punctuation.\n",
    "``set(text2)`` gives you a list of all the tokens in the text, without the duplicates.\n",
    "Hence, ``len(set(text3))`` will give you the total number unique tokens. Remember this still includes punctuation.\n",
    "``sorted(text4)`` places items in the list into alphabetical order, with punctuation symbols and capitalised words first.\n",
    "Please note that all these commands use the same syntax; this is the first python syntax we'll learn.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Jupyter as a calculator\n",
    "We can use the Jupyter environment as an overblown calculator; try doing some basic mathematics with python. Hint: use ``*`` and ``/`` like your smartphone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The expression that you just wrote above is the most basic programming instruction in the Python language. It includes values (the numbers) and operators (``* + -`` etc...) and always evaluate/reduce down to one value.\n",
    "\n",
    "_**Operators**_ are important and we can do more with them than multiply. We can ask Jupyter if something is equal to ``==`` or not equal to ``!=`` and a number of others. Try it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handy! This introduces us to _**boolean operators**_. Jupyter is telling me whether the expressions I've type are True or False. You can find more on boolean operators [here](https://docs.python.org/3/library/stdtypes.html?highlight=boolean%20operators).\n",
    "\n",
    "#### 1. Challenge!\n",
    "We can investigate the lexical richness of a text. For example, by dividing the total number of words by the number of unique words, we can see the average number of times each word is used.\n",
    "\n",
    "For this challenge you will have to combine your knowledge of the syntax we've learnt so far and Jupyter's mathematical abilities.\n",
    "\n",
    "Have a go at calculating the lexical richness of text3 i.e. the length or ``len()`` of ``text3`` divided by the length of the set or ``set()`` of ``text3``.\n",
    "\n",
    "We can also count the number of times a word is used and calculate what percentage of the text it represents. But to do this we need to learn some new syntax. Methods that use dot notation only work with _**strings**_; on the other hand, ``len()`` and ``sorted()`` can work on other data types. Fortunately, we're mostly interested in strings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try exchanging ``\"America\"`` with any word of your choice (note that it may not be in the corpus). Tip: you can tell that \"America\" is a string because it is surrounded by quotation marks, unlike, say, a variable, which has no quotation marks.\n",
    "\n",
    "#### 2. Challenge!\n",
    "How would you calculate the percentage of Text 4 that is taken up by the word \"America\"? For this you'll need to remember the mathematical formula to calculate percentages and the count method above. Think about how you would combine these two pieces of information if you were a computer..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring text - concordances, similar contexts, dispersion plots\n",
    "_**Concordancing**_ shows you a word in context and is useful if you want to be able to discuss the ways in which a word is used in a text. 'Similar' will find words used in similar contexts; remember it is not looking for synonyms, although the results may include synonyms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also find words that typically occur together, which tend to be very specific to a text or genre of texts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Challenge!\n",
    "- Find the collocations in a text of your choice.\n",
    "- Chose a word to concordance.\n",
    "- Investigate how the word is used. What words are used similarly?\n",
    "- And what are the common contexts of these words?\n",
    "- Use your phone or computer to take a picture and tweet your most interesting findings to [@resplat](https://twitter.com/resplat).\n",
    "\n",
    "Python also lets you create graphs to display data. To represent information about a text graphically, import the Python library numpy. We can then generate a dispersion plot that shows where given words occur in a text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "%matplotlib inline\n",
    "text1.dispersion_plot([\"whale\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.Challenge!\n",
    "Create a dispersion plot for the terms ``\"citizens\", \"democracy\", \"freedom\", \"duties\"`` and ``\"America\"`` in the inaugural address corpus. What do you think it tells you?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We can use Python's ability to perform statistical analysis of data to do further exploration of vocabulary. For instance, we might want to be able to find the most common or least common words in a text. We'll start by looking at frequency distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist\n",
    "from collections import Counter\n",
    "fdist1 = FreqDist(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Key Concepts: Tokenization\n",
    "In general, Python regards a text file as a single long string of characters. Tokenization breaks text into words that the computer can understand as discrete units. Here is an example of one of NLTK's tokenizers at work:\n",
    "\n",
    "First, we import the special twitter tokenizer from nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize.casual import (TweetTokenizer, casual_tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, save a tweet as a variable. In computer programming, variables are data (e.g. “PM @TurnbullMalcolm: Under changes agreed...”) paired with an associated symbolic name or identifier (e.g. 'tweet' in the code below). We'll learn more about these later, but here's how you assign data a variable. I've given my tweet the variable name 'tweet'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweet = \"PM @TurnbullMalcolm: Under changes agreed to today, it's 'inconceivable' Brighton terrorist would have got parole. @theheraldsun #auspol\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'Call' (programmer speak) your tweet to check it's saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use our special tweet tokenizer to tell the computer to recognise my tweet as a list of words, not a long string of characters. To do this a create and save another variable. This is common practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now call your new variable and observe how it differs from when you called the first variable 'tweet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_\n",
    "Compare the output of the variable sentence and the variable words. Notice that in the latter, the words are represented as a list._\n",
    "\n",
    "### _Challenge!_\n",
    "Try running `tweet[1]` and the `tweet_tokens[1]` in separate cells. Observe what happens. What unit is each variable count? Try changing the numbers in the square brackets. Have you noticed that Python starts counting at 0?\n",
    "Using the `casual_tokenize()` function of nltk has changed our sentence into a list of words that can be searched, rather than characters. We saved our initial sentence as ‘tweet’ and the list of tokenised words as ‘tweet_tokens’, using numbers within the square brackets allows us to ask the computer what value (character or word) is at a particular position in the list. This is called indexing. A list in computer programming is an abstract data type that represents a countable number of ordered values. You can learn more about list function and data structures in Python [here](https://docs.python.org/3/tutorial/datastructures.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Basics: Lists\n",
    "Python treats a text as a long list of words. First, we'll make some lists of our own, to give you an idea of how a list behaves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The opening sentences of each of our texts have been pre-defined for you. You can inspect them by typing in `sent2` etc.\n",
    "You can add lists together, creating a new list containing all the items from both lists. You can do this by typing out the two lists or you can add two or more pre-defined lists. This is called concatenation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also add an item to the end of a list by appending. When we ``append()``, the list itself is updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Indexing Lists\n",
    "We can navigate this list with the help of indexes. Just as we can find out the number of times a word occurs in a text, we can also find where a word first occurs. We can navigate to different points in a text without restriction, so long as we can describe where we want to be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As well as pulling out individual items from a list, indexes can be used to pull out selections of text from a large corpus to inspect. We call this slicing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we're asking for the beginning or end of a text, we can leave out the first or second number. For instance, [:5] will give us the first five items in a list while [8:] will give us all the elements from the eighth to the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the first element in the list is zero. This is because we are telling Python to go zero steps forward in the list. If we use an index that is too large (that is, we ask for something that doesn't exist), we'll get an error. We can modify elements in a list by assigning new data to one of its index values. We can also replace a slice with new material."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
