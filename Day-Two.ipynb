{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day Two: SESSION ONE\n",
    "\n",
    "Before we start, make sure you import nltk into your new notebook, as well as numpy and the example texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy\n",
    "%matplotlib inline\n",
    "import pylab\n",
    "from nltk.book import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenisation Revisted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload the file you saved yesterday to the Jupyter notebook. \n",
    "- Find the file in Jupyter (where you saved it in Excel) to discovery your 'directory path'. \n",
    "- For Mac go to the find and press command+shift+H for home for the name of your home directory. It should be a combination of your name from when you originally set up for computer. \n",
    "- Your pathway to the file you saved will be \"/User/YourHomeDirectory/Documents/YourFileName.txt\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save that directory as a string with a variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "textpath = \"/Users/kimdoyle/Documents/#auspol.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python's `os` module can be used to perform tasks such as finding the name of present working directory, changing current working directory, checking if certain files or directories exist at a location, creating new directories and much more!. Let's import os to help us read the file path we just set in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text = open(os.path.join(textpath), \"r\", encoding='UTF-16').read() #open and join a path to the file \n",
    "#'r' reads the file\n",
    "#you can print(text) to check this has worked\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Challenge!_\n",
    "You now have a twitter dataset of your own. Try to tokenize the data using the exact same steps as you used to tokenize the single tweet in the previous session. This time, your first variable is 'text', rather than 'tweet'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can see that the data is a list of words and punctuation. Fortunately, our special tweet tokeniser is clever enough to recognise that @ symbols are part of user names, although no tokeniser is perfect and you will most likely need to perform data cleaning on a big dataset. This is beyond the scope of this tutorial. However, we can remove punctuation, capitalised words and 'stopwords', to get more insights into our data. First, we need to understand how loops work..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Basics: Loops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use Python to automate tasks, such as performing a function on all items in a list. In our case, we want Python to check if each word in our tweet corpus is alphabetical and lower case. To do that, we'll have to teach the computer how to repeat things. We do this by creating something called a loop.\n",
    "\n",
    "This is the general form of a 'for' loop:\n",
    "\n",
    "    for item in collection:\n",
    "    do things with item\n",
    "\n",
    "Loops are used for lots of things in programming and there are many types of loops. Today we are only interested in for loops and our list of tweets.\n",
    "\n",
    "We are going to create a list and assign it a variable. We will use this variable in our loop to create fruit salad!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do this with a pre-exisiting list. For example, the pre-defined sentences that come with the nltk book data. Let's use `sent2`, which is the first sentence of `text1` Sense and Sensibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to write this is to save it as a new list and give it a variable name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use numerical operators to refine the types of searches we ask Python to run. We can use the following relational operators:\n",
    "\n",
    "\n",
    "### Common relationals\n",
    " |  Relational | Meaning |\n",
    " |--------------:|:------------|\n",
    " | <    |  less than |\n",
    " | <=   |   less than or equal to |\n",
    " | ==  |    equal to (note this is two \"=\" signs, not one) |\n",
    " | !=   |   not equal to |\n",
    " | \\>   |   greater than |\n",
    " | \\>= |   greater than or equal to |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Challenge!_\n",
    "Using one of the pre-defined sentences in the NLTK corpus, use the relational operators above to find:\n",
    "- Words of four or more characters\n",
    "- Words of exactly four characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Concepts: Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common operators\n",
    "\n",
    " | Operator  | Purpose  |\n",
    " |--------------|------------|\n",
    " | s.startswith(t) | test if s starts with t |\n",
    " | s.endswith(t)  |  test if s ends with t | \n",
    " | t in s         |  test if t is a substring of s | \n",
    " | s.islower()    |  test if s contains cased characters and all are lowercase | \n",
    " | s.isupper()    |  test if s contains cased characters and all are uppercase | \n",
    " | s.isalpha()    |  test if s is non-empty and all characters in s are alphabetic | \n",
    " | s.isalnum()    |  test if s is non-empty and all characters in s are alphanumeric | \n",
    " | s.isdigit()    |  test if s is non-empty and all characters in s are digits | \n",
    " | s.istitle()    |  test if s contains cased characters and is titlecased (i.e. all words in s have initial capitals) | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to remove punctuation and capitalised words from our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that this removes punctuation, but also all the user names and hashtags. We can write a better loop that includes these important features, but still excludes punctuation using `s.startswith(t)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python still thinks lower case words and capitalised words are different words e.g. 'Left' and 'left' would be considered two different words. We can change this by making all the words lowercase using the `s.lower()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try running some NLTK operations on the data, such as `something.collocations()` what happens?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform special NLTK tasks like `.concordance(), .collocations()` and `.similar()`, we need to convert our 'list' in an NLTK Text. Forunately, this is fairly straightforward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try collocations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Concepts: Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typically, a lot of space is taken up in texts by little words like 'and' and 'of' and 'the' which don't add a lot to our understanding of text. These are called 'stop words'. It will help our analysis if we exclude them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Challenge!_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are interested in the most common words, excluding hashtags and usernames, you can use the old `alpha` variable we created. \n",
    "- remove capitalised letter from alpha\n",
    "- Make sure you convert it to NLTK Text first and save it to a new variable\n",
    "- Remove stopwords\n",
    "- Run the FreqDist function\n",
    "- Find the `most.common()[:20]` words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day Two: SESSION 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Challenge!_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to experiment with your team!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
