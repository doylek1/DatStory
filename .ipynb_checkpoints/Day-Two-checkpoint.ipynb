{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day Two: PART ONE\n",
    "\n",
    "Before we start, make sure you import nltk into your new notebook, as well as numpy and the example texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy\n",
    "%matplotlib inline\n",
    "from nltk.book import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Concepts: Tokenization\n",
    "In general, Python regards a text file as a single long string of characters. Tokenization breaks text into words that the computer can understand as discrete units. Here is an example of one of NLTK's tokenizers at work:\n",
    "\n",
    "First, we import the special twitter tokenizer from nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize.casual import (TweetTokenizer, casual_tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, save a tweet as a variable. In computer programming, variables are data (e.g. “PM @TurnbullMalcolm: Under changes agreed...”) paired with an associated symbolic name or identifier (e.g. 'tweet' in the code below). We'll learn more about these later, but here's how you assign data a variable. I've given my tweet the variable name 'tweet'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweet = \"PM @TurnbullMalcolm: Under changes agreed to today, it's 'inconceivable' Brighton terrorist would have got parole. @theheraldsun #auspol\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'Call' (programmer speak) your tweet to check it's saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"PM @TurnbullMalcolm: Under changes agreed to today, it's 'inconceivable' Brighton terrorist would have got parole. @theheraldsun #auspol\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use our special tweet tokenizer to tell the computer to recognise my tweet as a list of words, not a long string of characters. To do this a create and save another variable. This is common practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweet_tokens = casual_tokenize(tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now call your new variable and observe how it differs from when you called the first variable 'tweet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PM',\n",
       " '@TurnbullMalcolm',\n",
       " ':',\n",
       " 'Under',\n",
       " 'changes',\n",
       " 'agreed',\n",
       " 'to',\n",
       " 'today',\n",
       " ',',\n",
       " \"it's\",\n",
       " \"'\",\n",
       " 'inconceivable',\n",
       " \"'\",\n",
       " 'Brighton',\n",
       " 'terrorist',\n",
       " 'would',\n",
       " 'have',\n",
       " 'got',\n",
       " 'parole',\n",
       " '.',\n",
       " '@theheraldsun',\n",
       " '#auspol']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_\n",
    "Compare the output of the variable sentence and the variable words. Notice that in the latter, the words are represented as a list._\n",
    "\n",
    "### _Challenge!_\n",
    "Try running `tweet[1]` and the `tweet_tokens[1]` in separate cells. Observe what happens. What unit is each variable count? Try changing the numbers in the square brackets. Have you noticed that Python starts counting at 0?\n",
    "Using the `casual_tokenize()` function of nltk has changed our sentence into a list of words that can be searched, rather than characters. We saved our initial sentence as ‘tweet’ and the list of tokenised words as ‘tweet_tokens’, using numbers within the square brackets allows us to ask the computer what value (character or word) is at a particular position in the list. This is called indexing. A list in computer programming is an abstract data type that represents a countable number of ordered values. You can learn more about list function and data structures in Python [here](https://docs.python.org/3/tutorial/datastructures.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'M'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'@TurnbullMalcolm'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_tokens[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PM'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_tokens[0] #noticed that Python starts counting at 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Basics: Lists\n",
    "Python treats a text as a long list of words. First, we'll make some lists of our own, to give you an idea of how a list behaves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent1\n",
    "len(sent1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The opening sentences of each of our texts have been pre-defined for you. You can inspect them by typing in `sent2` etc.\n",
    "You can add lists together, creating a new list containing all the items from both lists. You can do this by typing out the two lists or you can add two or more pre-defined lists. This is called concatenation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Fellow',\n",
       " '-',\n",
       " 'Citizens',\n",
       " 'of',\n",
       " 'the',\n",
       " 'Senate',\n",
       " 'and',\n",
       " 'of',\n",
       " 'the',\n",
       " 'House',\n",
       " 'of',\n",
       " 'Representatives',\n",
       " ':',\n",
       " 'Call',\n",
       " 'me',\n",
       " 'Ishmael',\n",
       " '.']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent4 + sent1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also add an item to the end of a list by appending. When we ``append()``, the list itself is updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Call', 'me', 'Ishmael', '.', 'Please']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent1.append('Please')\n",
    "sent1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Indexing Lists\n",
    "We can navigate this list with the help of indexes. Just as we can find out the number of times a word occurs in a text, we can also find where a word first occurs. We can navigate to different points in a text without restriction, so long as we can describe where we want to be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "173\n"
     ]
    }
   ],
   "source": [
    "print(text4.index('awaken'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This works in reverse as well. We can ask Python to locate the 158th item in our list (note that we use square brackets here, not parentheses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "awaken\n"
     ]
    }
   ],
   "source": [
    "print(text4[173])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As well as pulling out individual items from a list, indexes can be used to pull out selections of text from a large corpus to inspect. We call this slicing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['U86', 'thats', 'why', 'something', 'like', 'gamefly', 'is', 'so', 'good', 'because', 'you', 'can', 'actually', 'play', 'a', 'full', 'game', 'without', 'buying', 'it']\n"
     ]
    }
   ],
   "source": [
    "print(text5[16715:16735])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we're asking for the beginning or end of a text, we can leave out the first or second number. For instance, [:5] will give us the first five items in a list while [8:] will give us all the elements from the eighth to the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[', 'Sense', 'and', 'Sensibility', 'by', 'Jane', 'Austen', '1811', ']', 'CHAPTER']\n",
      "['upon', 'us', ',', 'we', 'carried', 'forth', 'that', 'great', 'gift', 'of', 'freedom', 'and', 'delivered', 'it', 'safely', 'to', 'future', 'generations', '.', 'Thank', 'you', '.', 'God', 'bless', 'you', '.', 'And', 'God', 'bless', 'the', 'United', 'States', 'of', 'America', '.']\n"
     ]
    }
   ],
   "source": [
    "print(text2[:10])\n",
    "print(text4[145700:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To help you understand how indexes work, let's create one.\n",
    "We start by defining the name of our index and then add the items. You probably won't do this in your own work, but you may want to manipulate an index in other ways. Pay attention to the quote marks and commas when you create your test sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The\n",
      "brown\n"
     ]
    }
   ],
   "source": [
    "sent = ['The', 'quick', 'brown', 'fox']\n",
    "print(sent[0])\n",
    "print(sent[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the first element in the list is zero. This is because we are telling Python to go zero steps forward in the list. If we use an index that is too large (that is, we ask for something that doesn't exist), we'll get an error.\n",
    "We can modify elements in a list by assigning new data to one of its index values. We can also replace a slice with new material."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'quick', 'furry', 'child']\n"
     ]
    }
   ],
   "source": [
    "sent[2] = 'furry'\n",
    "sent[3] = 'child'\n",
    "print(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Concepts: Tokenization Revisited\n",
    "Now that we know how to tokenize tweets and how to handle lists, we can try tokenizing our own data.\n",
    "Got to your TAGS Google Sheet on your drive and copy the 'text' section into a blank excel file like so.\n",
    "![](http://localhost:8888/files/Documents/DatStory/images/GoogleSheet.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the data in your Documents as a text file.\n",
    "![](http://localhost:8888/files/Documents/DatStory/images/SaveAs.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, upload to file to Jupyter. \n",
    "- Find the file in Jupyter (where you saved it in Excel) to discovery your 'directory path'. \n",
    "- For Mac go to the find and press command+shift+H for home for the name of your home directory. It should be a combination of your name from when you originally set up for computer. \n",
    "- Your pathway to the file you saved will be \"/User/YourHomeDirectory/Documents/YourFileName.txt\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save that directory as a string with a variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "textpath = \"/Users/kimdoyle/Documents/#auspol.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python's `os` module can be used to perform tasks such as finding the name of present working directory, changing current working directory, checking if certain files or directories exist at a location, creating new directories and much more!. Let's import os to help us read the file path we just set in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os \n",
    "file = open(os.path.join(textpath), \"r\", encoding='UTF-16') \n",
    "text = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text = open(os.path.join(textpath), \"r\", encoding='UTF-16').read() #open and join a path to the file, 'r' reads the file\n",
    "#you can print(text) to check this has worked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Challenge!_\n",
    "You now have a twitter dataset of your own. Try to tokenize the data using the exact same steps as you used to tokenize the single tweet in the previous session. This time, your first variable is 'text', rather than 'tweet'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\"', 'RT', '@JohnWren1950', ':', '\"', '\"', 'left', 'wing', 'fascists', '\"']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize.casual import (TweetTokenizer, casual_tokenize) #make sure the tokenizers are already installed\n",
    "tweet_tokens = casual_tokenize(text) #you can call your tokenized tweets whatever you like. I went with 'tweet_tokens'\n",
    "tweet_tokens[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can see that the data is a list of words and punctuation. Fortunately, our special tweet tokeniser is clever enough to recognise that @ symbols are part of user names, although no tokeniser is perfect and you will most likely need to perform data cleaning on a big dataset. This is beyond the scope of this tutorial. However, we can remove punctuation, capitalised words and 'stopwords', to get more insights into our data. First, we need to understand how loops work..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Basics: Loops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use Python to automate tasks, such as performing a function on all items in a list. In our case, we want Python to check if each word in our tweet corpus is alphabetical and lower case. To do that, we'll have to teach the computer how to repeat things. We do this by creating something called a loop.\n",
    "\n",
    "This is the general form of a 'for' loop:\n",
    "\n",
    "    for item in collection:\n",
    "    do things with item\n",
    "\n",
    "Loops are used for lots of things in programming and there are many types of loops. Today we are only interested in for loops and our list of tweets.\n",
    "\n",
    "We are going to create a list and assign it a variable. We will use this variable in our loop to create fruit salad!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current fruit : banana\n",
      "Current fruit : apple\n",
      "Current fruit : mango\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "fruits = ['banana', 'apple', 'mango']  \n",
    "for fruit in fruits: #our item is fruit; our collection is fruits \n",
    "    print('Current fruit :', fruit) #everything in the loop is printed till there are no more items\n",
    "print('Done!') #after that, anything outside the loop is printed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do this with a pre-exisiting list. For example, the pre-defined sentences that come with the nltk book data. Let's use `sent2`, which is the first sentence of `text1` Sense and Sensibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'family',\n",
       " 'of',\n",
       " 'Dashwood',\n",
       " 'had',\n",
       " 'long',\n",
       " 'been',\n",
       " 'settled',\n",
       " 'in',\n",
       " 'Sussex',\n",
       " '.']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "family\n",
      "Dashwood\n",
      "settled\n",
      "Sussex\n"
     ]
    }
   ],
   "source": [
    "for word in sent2:\n",
    "    if len(word) > 4: #print words in sent2 if they are greater than 4\n",
    "        print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to write this is to save it as a new list and give it a variable name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['family', 'Dashwood', 'settled', 'Sussex']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "longer = [word for word in sent2 if len(word) > 4]\n",
    "longer #call the new variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use numerical operators to refine the types of searches we ask Python to run. We can use the following relational operators:\n",
    "\n",
    "\n",
    "### Common relationals\n",
    " |  Relational | Meaning |\n",
    " |--------------:|:------------|\n",
    " | <    |  less than |\n",
    " | <=   |   less than or equal to |\n",
    " | ==  |    equal to (note this is two \"=\" signs, not one) |\n",
    " | !=   |   not equal to |\n",
    " | \\>   |   greater than |\n",
    " | \\>= |   greater than or equal to |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Challenge!_\n",
    "Using one of the pre-defined sentences in the NLTK corpus, use the relational operators above to find:\n",
    "- Words of four or more characters\n",
    "- Words of exactly four characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['family', 'Dashwood', 'settled', 'Sussex'] ['family', 'Dashwood', 'long', 'been', 'settled', 'Sussex'] ['long', 'been']\n"
     ]
    }
   ],
   "source": [
    "more = [word for word in sent2 if len(word) >= 4]\n",
    "exact = [word for word in sent2 if len(word) == 4]\n",
    "print(longer, more, exact)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Concepts: Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common operators\n",
    "\n",
    " | Operator  | Purpose  |\n",
    " |--------------|------------|\n",
    " | s.startswith(t) | test if s starts with t |\n",
    " | s.endswith(t)  |  test if s ends with t | \n",
    " | t in s         |  test if t is a substring of s | \n",
    " | s.islower()    |  test if s contains cased characters and all are lowercase | \n",
    " | s.isupper()    |  test if s contains cased characters and all are uppercase | \n",
    " | s.isalpha()    |  test if s is non-empty and all characters in s are alphabetic | \n",
    " | s.isalnum()    |  test if s is non-empty and all characters in s are alphanumeric | \n",
    " | s.isdigit()    |  test if s is non-empty and all characters in s are digits | \n",
    " | s.istitle()    |  test if s contains cased characters and is titlecased (i.e. all words in s have initial capitals) | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to remove punctuation and capitalised words from our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RT',\n",
       " 'left',\n",
       " 'wing',\n",
       " 'fascists',\n",
       " 'LOL',\n",
       " 'Bless',\n",
       " 'UK',\n",
       " 'Election',\n",
       " 'UKIP',\n",
       " 'London']"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha = [word for word in tweet_tokens if word.isalpha()]\n",
    "alpha[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that this removes punctuation, but also all the user names and hashtags. We can write a better loop that includes these important features, but still excludes punctuation using `s.startswith(t)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RT',\n",
       " '@JohnWren1950',\n",
       " 'left',\n",
       " 'wing',\n",
       " 'fascists',\n",
       " 'LOL',\n",
       " 'Bless',\n",
       " '#auspol',\n",
       " '#RWNJ',\n",
       " 'UK']"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha2 = [word for word in tweet_tokens if word.isalpha() or word.startswith('@') or word.startswith('#')]\n",
    "alpha2[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python still thinks lower case words and capitalised words are different words e.g. 'Left' and 'left' would be considered two different words. We can change this by making all the words lowercase using the `s.lower()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lowered = [word.lower() for word in alpha2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try running some NLTK operations on the data, such as `something.collocations()` what happens?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'collocation'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-122-99fe26492706>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlowered\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollocation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'collocation'"
     ]
    }
   ],
   "source": [
    "lowered.collocation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'nltk.text.Text'>\n"
     ]
    }
   ],
   "source": [
    "print(type(lowered))\n",
    "print(type(text1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform special NLTK tasks like `.concordance(), .collocations()` and `.similar()`, we need to convert our 'list' in an NLTK Text. Forunately, this is fairly straightforward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nltk.text.Text"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import Text\n",
    "nltk_text = Text(lowered)\n",
    "type(nltk_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try collocations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ukip #bxb1; #votelabour #forthemany; london #londonbridgeattack;\n",
      "#forthemany @jeremycorbyn; @jeremycorbyn london; #coag ukip; #bxb1\n",
      "#ge2017; #ge2017 #votelabour; election #coag; climate change; #uspoli\n",
      "@potus; key lesson; @unionsaustralia key; entirely due; vested\n",
      "interests; failed working; #maga #news; lost years; working people;\n",
      "election economics\n"
     ]
    }
   ],
   "source": [
    "nltk_text.collocations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Concepts: Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typically, a lot of space is taken up in texts by little words like 'and' and 'of' and 'the' which don't add a lot to our understanding of text. These are called 'stop words'. It will help our analysis if we exclude them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('#auspol', 2568),\n",
       " ('rt', 1567),\n",
       " ('the', 1180),\n",
       " ('to', 882),\n",
       " ('a', 541),\n",
       " ('and', 528),\n",
       " ('is', 510),\n",
       " ('of', 452),\n",
       " ('#ge2017', 450),\n",
       " ('for', 398),\n",
       " ('election', 396),\n",
       " ('uk', 376),\n",
       " ('in', 363),\n",
       " ('on', 351),\n",
       " ('#coag', 329),\n",
       " ('@jeremycorbyn', 241),\n",
       " ('london', 237),\n",
       " ('#forthemany', 237),\n",
       " ('ukip', 236),\n",
       " ('#londonbridgeattack', 236)]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdist1 = nltk.FreqDist(nltk_text)\n",
    "fdist1.most_common()[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('#auspol', 2568),\n",
       " ('#ge2017', 450),\n",
       " ('election', 396),\n",
       " ('uk', 376),\n",
       " ('#coag', 329),\n",
       " ('@jeremycorbyn', 241),\n",
       " ('london', 237),\n",
       " ('#forthemany', 237),\n",
       " ('ukip', 236),\n",
       " ('#londonbridgeattack', 236),\n",
       " ('#votelabour', 236),\n",
       " ('#bxb1', 236),\n",
       " ('change', 191),\n",
       " ('climate', 175),\n",
       " ('people', 154),\n",
       " ('turnbull', 99),\n",
       " ('failed', 98),\n",
       " ('australia', 97),\n",
       " ('#uspoli', 96),\n",
       " ('policy', 95)]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "#Create a variable that contains all the stopwords in your tweet corpus\n",
    "stopped = nltk.corpus.stopwords.words('english') + ['rt', 'via']\n",
    "unstopped = [word for word in nltk_text if word not in stopped]\n",
    "fdist2 = nltk.FreqDist(unstopped)\n",
    "fdist2.most_common()[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Challenge!_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are interested in the most common words, excluding hashtags and usernames, you can use the old `alpha` variable we created. \n",
    "- remove capitalised letter from alpha\n",
    "- Make sure you convert it to NLTK Text first and save it to a new variable\n",
    "- Remove stopwords\n",
    "- Run the FreqDist function\n",
    "- Find the `most.common()[:20]` words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lowered_alpha = [word.lower() for word in alpha]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rt',\n",
       " 'left',\n",
       " 'wing',\n",
       " 'fascists',\n",
       " 'lol',\n",
       " 'bless',\n",
       " 'uk',\n",
       " 'election',\n",
       " 'ukip',\n",
       " 'london']"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk_alpha = Text(lowered_alpha)\n",
    "nltk_alpha[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('election', 396),\n",
       " ('uk', 376),\n",
       " ('london', 237),\n",
       " ('ukip', 236),\n",
       " ('change', 191),\n",
       " ('climate', 175),\n",
       " ('people', 154),\n",
       " ('turnbull', 99),\n",
       " ('failed', 98),\n",
       " ('australia', 97),\n",
       " ('policy', 95),\n",
       " ('gas', 87),\n",
       " ('know', 83),\n",
       " ('malcolm', 82),\n",
       " ('working', 80),\n",
       " ('may', 80),\n",
       " ('years', 79),\n",
       " ('comey', 78),\n",
       " ('time', 77),\n",
       " ('key', 76)]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unstopped = [word for word in nltk_alpha if word not in stopped]\n",
    "fdist3 = nltk.FreqDist(unstopped)\n",
    "fdist3.most_common()[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Challenge!_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to experiment with your team!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
